# Text Summarization
[Blog cá»§a Google vá» Model vÃ  Dataset](https://research.google/blog/exploring-transfer-learning-with-t5-the-text-to-text-transfer-transformer/).  
[Trang thÃ´ng tin chi tiáº¿t vá» Dataset](https://huggingface.co/datasets/billsum).  
## Dataset
Dataset Ä‘Æ°á»£c sá»­ dá»¥ng lÃ  dataset BillSum, gá»“m cÃ¡c dá»± luáº­t cá»§a quá»‘c há»™i Má»¹ vÃ  bang California (â€œUS Congressional and California state billsâ€) vÃ  tÃ³m táº¯t cá»§a chÃºng.  
Bao gá»“m cÃ¡c Ä‘áº·c trÆ°ng: Ná»™i dung cá»§a Bill, tÃ³m táº¯t, tiÃªu Ä‘á» (chá»‰ cÃ³ vá»›i dá»± luáº­t quá»‘c há»™i, khÃ´ng cÃ³ vá»›i California), Ä‘á»™ dÃ i cá»§a ná»™i dung, Ä‘á»™ dÃ i tÃ³m táº¯t.  

![](./report_data/BillSumImg.png)  
*áº¢nh chá»¥p má»™t pháº§n dá»¯ liá»‡u gá»‘c chÆ°a qua tiá»n xá»­ lÃ½.*  

## Tiá»n xá»­ lÃ½ dataset  
```
from transformers import AutoTokenizer
tokenizer = AutoTokenizer.from_pretrained("google-t5/t5-small")
```  
Dá»¯ liá»‡u Ä‘Æ°á»£c tokenized theo nhÆ° bÃ¡o cÃ¡o cá»§a mÃ´ hÃ¬nh gá»‘c, bao gá»“m cÃ¡c bÆ°á»›c:  
	Má»™t  
	Hai  
	Ba  
	Bá»‘n  
	NÄƒm  
test
## Model  
Model Ä‘Æ°á»£c sá»­ dá»¥ng lÃ  *google-t5/t5-small* bá»Ÿi Google.  
Má»¥c tiÃªu cá»§a há» khi táº¡o ra model nÃ y lÃ  dÃ¹ng nÃ³ cho nhiá»u tÃ¡c vá»¥ khÃ¡c nhau vá»›i má»™t model duy nháº¥t, má»™t metric duy nháº¥t, má»™t thuáº­t toÃ¡n tá»‘i Æ°u duy nháº¥t nháº±m Ä‘Æ¡n giáº£n hÃ³a quÃ¡ trÃ¬nh so sÃ¡nh giá»¯a cÃ¡c bá»™ dá»¯ liá»‡u vÃ  cÃ¡c bÃ i toÃ¡n khÃ¡c nhau.
NÃ³ lÃ  má»™t model á»©ng dá»¥ng Transformer vá»›i input/output Ä‘á»u lÃ  vÄƒn báº£n (KhÃ¡c vá»›i model Bart, cÅ©ng cá»§a Google).  
Tuy nhiÃªn vá» cÃ¡i cá»‘t lÃµi cá»§a Transformer thÃ¬ khÃ´ng cÃ³ sá»± thay Ä‘á»•i Ä‘Ã¡ng ká»ƒ nÃªn nhÃ³m tÃ¡c giáº£ Ä‘Ã£ hÆ°á»›ng ngÆ°á»i Ä‘á»c tá»›i bÃ¡o cÃ¡o gá»‘c cá»§a Transformer Ä‘á»ƒ hiá»ƒu rÃµ hÆ¡n.  
  
"We use a simplified form of position embeddings
where each â€œembeddingâ€ is simply a scalar that is added to the corresponding logit used
for computing the attention weights. For efficiency, we also share the position embedding
parameters across all layers in our model, though within a given layer each attention head
uses a different learned position embedding"
"To summarize, our model is roughly equivalent to the original Transformer proposed by
Vaswani et al. (2017) with the exception of removing the Layer Norm bias, placing the layer
normalization outside the residual path, and using a different position embedding scheme.
Since these architectural changes are orthogonal to the experimental factors we consider in
our empirical survey of transfer learning, we leave the ablation of their impact for future
work."
  
ÄÃ¢y lÃ  há»i chatGpt Ä‘á»ƒ Ä‘á»c, mÃ  tháº¥y khÃ´ng Ä‘Ã¡ng tin tÃ­ nÃ o cáº£ nÃªn thÃ´i cháº¯c xÃ³a pháº§n nÃ y rá»“i Ä‘á»c paper cho cháº¯c  
"Certainly! The architecture of the Text-to-Text Transfer Transformer (T5) model is based on the Transformer architecture, which has been highly successful in natural language processing (NLP) tasks. Here are the key components:

Encoder-Decoder Structure:
T5 follows the standard Transformer architecture, consisting of an encoder and a decoder.
The encoder processes the input text, while the decoder generates the output text.
Both encoder and decoder consist of multiple layers of self-attention and feed-forward neural networks.
Unified Text-to-Text Format:
T5 reframes all NLP tasks into a consistent format: both input and output are treated as text strings.
This allows T5 to handle a wide range of tasks without task-specific modifications.
Pre-training and Fine-tuning:
T5 is pre-trained on a large corpus of text data using unsupervised learning.
During pre-training, it learns to predict masked tokens (similar to BERT) and also learns to generate text.
Fine-tuning involves training T5 on specific downstream tasks with labeled data.
Task-Specific Heads:
T5 uses task-specific heads (output layers) for different tasks.
For example, a translation task would have a different head than a summarization task.
Shared Vocabulary:
T5 uses a shared vocabulary for both input and output tokens.
This simplifies the model and allows it to handle diverse tasks.
Task Prefixes:
T5 introduces task-specific prefixes to guide the model during fine-tuning.
For instance, a translation task might have the prefix â€œTranslate English to French: â€¦â€
Overall, T5â€™s strength lies in its versatilityâ€”it can be fine-tuned for various NLP tasks by simply adjusting the task-specific head and providing appropriate prefixes during fine-tuning. ğŸ˜ŠğŸš€

If you have any more questions or need further details, feel free to ask!"
  
  
### Pre-train dataset
Model Ä‘Ã£ Ä‘Æ°á»£c pre-train trÃªn táº­p dataset *Colossal Clean Crawled Corpus (C4)*, vá»‘n lÃ  má»™t táº­p dá»¯ liá»‡u vÄƒn báº£n khá»•ng lá»“ Ä‘Æ°á»£c crawl tá»« dá»¯ liá»‡u kháº¯p nÆ¡i trÃªn internet.  
NhÃ³m Google tháº¥y táº­p Ä‘Ã³ cÃ³ sá»‘ lÆ°á»£ng lá»›n nhÆ°ng cháº¥t lÆ°á»£ng cá»±c tháº¥p nÃªn há» Ä‘Ã£ tiáº¿n hÃ nh "lá»c" láº¡i bá»™ dá»¯ liá»‡u nhÆ° sau:  
"""
â€¢ Chá»‰ giá»¯ nhá»¯ng cÃ¢u káº¿t thÃºc báº±ng dáº¥u cÃ¢u. (LÃ : "? . !")  
â€¢ Loáº¡i bá» cÃ¡c trang web Ã­t hÆ¡n 3 cÃ¢u, chá»‰ giá»¯ nhá»¯ng cÃ¢u dÃ i hÆ¡n 4 chá»¯.  
â€¢ Loáº¡i bá» cÃ¡c trang cÃ³ nhá»¯ng tá»« "nháº¡y cáº£m".
â€¢ Nhiá»u trang Ä‘Æ°a ra cáº£nh bÃ¡o cáº§n báº­t Javascript nÃªn há» loáº¡i bá» nhá»¯ng cÃ¢u nÃ o cÃ³ chá»¯ "Javascript".
â€¢ Loáº¡i bá» cÃ¡c trang cÃ³ cá»¥m tá»« "Lorem Ipsum" vÃ¬ nÃ³ lÃ  vÄƒn máº«u Ä‘á»ƒ táº¡m.
â€¢ Má»™t sá»‘ trang web sáº½ pháº£i chá»©a code. VÃ¬ ngoáº·c nhá»n "{" Ä‘Æ°á»£c dÃ¹ng trong nhiá»u ngÃ´n ngá»¯ láº­p trÃ¬nh (nhÆ° Javascript, vá»‘n Ä‘Æ°á»£c dÃ¹ng ráº¥t nhiá»u trÃªn web),  loáº¡i bá» trang nÃ o chá»©a ngoáº·c nhá»n.  
â€¢ Loáº¡i bá» cÃ¡c kÃ½ hiá»‡u dáº«n nguá»“n (NhÆ° cá»§a Wikipedia lÃ  "[1], [Cáº§n dáº«n chá»©ng],...") nÃªn loáº¡i bá» háº¿t cÃ¡c kÃ½ hiá»‡u Ä‘Ã³.  
â€¢ Many pages had boilerplate policy notices, so we removed any lines containing the
strings â€œterms of useâ€, â€œprivacy policyâ€, â€œcookie policyâ€, â€œuses cookiesâ€, â€œuse of
cookiesâ€, or â€œuse cookiesâ€.
â€¢ Äá»ƒ giáº£m sá»± trÃ¹ng láº·p cho bá»™ dá»¯ liá»‡u, tiáº¿n hÃ nh loáº¡i bá» cÃ¡c bá»™ 3 cÃ¢u liÃªn tiáº¿p bá»‹ trÃ¹ng trong cáº£ bá»™ dá»¯ liá»‡u, giá»¯ láº¡i Ä‘Ãºng má»™t bá»™.
## Metric  
Metric Ä‘Æ°á»£c sá»­ dá»¥ng lÃ  Rogue Metric.


## Káº¿t quáº£ huáº¥n luyá»‡n  
KhÃ¡ tá»‘t, so sÃ¡nh trá»±c tiáº¿p vá»›i káº¿t quáº£ cá»§a model cÃ³ sáºµn thÃ¬ cho tháº¥y nÃ³ output giá»‘ng nhau.  
Cho tháº¥y quÃ¡ trÃ¬nh huáº¥n luyá»‡n Ä‘Ã£ thÃ nh cÃ´ng. 
![](./report_data/OutputDemo.png)  
*áº¢nh chá»¥p so sÃ¡nh giá»¯a output cá»§a mÃ´ hÃ¬nh huáº¥n luyá»‡n Ä‘Æ°á»£c (3) vÃ  mÃ´ hÃ¬nh cÃ³ sáºµn (2)*  