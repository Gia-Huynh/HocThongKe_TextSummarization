# Text Summarization
[Blog cá»§a Google vá» Model vÃ  Dataset](https://research.google/blog/exploring-transfer-learning-with-t5-the-text-to-text-transfer-transformer/).  
[Trang thÃ´ng tin chi tiáº¿t vá» Dataset](https://huggingface.co/datasets/billsum).  
## Dataset
Dataset Ä‘Æ°á»£c sá»­ dá»¥ng lÃ  dataset BillSum, gá»“m cÃ¡c dá»± luáº­t cá»§a quá»‘c há»™i Má»¹ vÃ  bang California (â€œUS Congressional and California state billsâ€) vÃ  tÃ³m táº¯t cá»§a chÃºng.  
Bao gá»“m cÃ¡c Ä‘áº·c trÆ°ng: Ná»™i dung cá»§a Bill, tÃ³m táº¯t, tiÃªu Ä‘á» (chá»‰ cÃ³ vá»›i dá»± luáº­t quá»‘c há»™i, khÃ´ng cÃ³ vá»›i California), Ä‘á»™ dÃ i cá»§a ná»™i dung, Ä‘á»™ dÃ i tÃ³m táº¯t.  

![](./report_data/BillSumImg.png)  
*áº¢nh chá»¥p má»™t pháº§n dá»¯ liá»‡u gá»‘c chÆ°a qua tiá»n xá»­ lÃ½.*  

## Tiá»n xá»­ lÃ½ dataset  
```
from transformers import AutoTokenizer
tokenizer = AutoTokenizer.from_pretrained("google-t5/t5-small")
```  
Dá»¯ liá»‡u Ä‘Æ°á»£c tokenized theo nhÆ° bÃ¡o cÃ¡o cá»§a mÃ´ hÃ¬nh gá»‘c, bao gá»“m cÃ¡c bÆ°á»›c:  
	Má»™t  
	Hai  
	Ba  
	Bá»‘n  
	NÄƒm  
test
## Model  
Model Ä‘Æ°á»£c sá»­ dá»¥ng lÃ  *google-t5/t5-small* bá»Ÿi Google.  
Má»¥c tiÃªu cá»§a há» khi táº¡o ra model nÃ y lÃ  dÃ¹ng nÃ³ cho nhiá»u tÃ¡c vá»¥ khÃ¡c nhau vá»›i má»™t model duy nháº¥t, má»™t metric duy nháº¥t, má»™t thuáº­t toÃ¡n tá»‘i Æ°u duy nháº¥t nháº±m Ä‘Æ¡n giáº£n hÃ³a quÃ¡ trÃ¬nh so sÃ¡nh giá»¯a cÃ¡c bá»™ dá»¯ liá»‡u vÃ  cÃ¡c bÃ i toÃ¡n khÃ¡c nhau.
NÃ³ lÃ  má»™t model á»©ng dá»¥ng Transformer vá»›i input/output Ä‘á»u lÃ  vÄƒn báº£n (KhÃ¡c vá»›i model Bart, cÅ©ng cá»§a Google).  
Tuy nhiÃªn vá» cÃ¡i cá»‘t lÃµi cá»§a Transformer thÃ¬ khÃ´ng cÃ³ sá»± thay Ä‘á»•i Ä‘Ã¡ng ká»ƒ nÃªn nhÃ³m tÃ¡c giáº£ Ä‘Ã£ hÆ°á»›ng ngÆ°á»i Ä‘á»c tá»›i bÃ¡o cÃ¡o gá»‘c cá»§a Transformer Ä‘á»ƒ hiá»ƒu rÃµ hÆ¡n.  
  
"We use a simplified form of position embeddings
where each â€œembeddingâ€ is simply a scalar that is added to the corresponding logit used
for computing the attention weights. For efficiency, we also share the position embedding
parameters across all layers in our model, though within a given layer each attention head
uses a different learned position embedding"
"To summarize, our model is roughly equivalent to the original Transformer proposed by
Vaswani et al. (2017) with the exception of removing the Layer Norm bias, placing the layer
normalization outside the residual path, and using a different position embedding scheme.
Since these architectural changes are orthogonal to the experimental factors we consider in
our empirical survey of transfer learning, we leave the ablation of their impact for future
work."
  
ÄÃ¢y lÃ  há»i chatGpt Ä‘á»ƒ Ä‘á»c, mÃ  tháº¥y khÃ´ng Ä‘Ã¡ng tin tÃ­ nÃ o cáº£ nÃªn thÃ´i cháº¯c xÃ³a pháº§n nÃ y rá»“i Ä‘á»c paper cho cháº¯c  
"Certainly! The architecture of the Text-to-Text Transfer Transformer (T5) model is based on the Transformer architecture, which has been highly successful in natural language processing (NLP) tasks. Here are the key components:

Encoder-Decoder Structure:
T5 follows the standard Transformer architecture, consisting of an encoder and a decoder.
The encoder processes the input text, while the decoder generates the output text.
Both encoder and decoder consist of multiple layers of self-attention and feed-forward neural networks.
Unified Text-to-Text Format:
T5 reframes all NLP tasks into a consistent format: both input and output are treated as text strings.
This allows T5 to handle a wide range of tasks without task-specific modifications.
Pre-training and Fine-tuning:
T5 is pre-trained on a large corpus of text data using unsupervised learning.
During pre-training, it learns to predict masked tokens (similar to BERT) and also learns to generate text.
Fine-tuning involves training T5 on specific downstream tasks with labeled data.
Task-Specific Heads:
T5 uses task-specific heads (output layers) for different tasks.
For example, a translation task would have a different head than a summarization task.
Shared Vocabulary:
T5 uses a shared vocabulary for both input and output tokens.
This simplifies the model and allows it to handle diverse tasks.
Task Prefixes:
T5 introduces task-specific prefixes to guide the model during fine-tuning.
For instance, a translation task might have the prefix â€œTranslate English to French: â€¦â€
Overall, T5â€™s strength lies in its versatilityâ€”it can be fine-tuned for various NLP tasks by simply adjusting the task-specific head and providing appropriate prefixes during fine-tuning. ğŸ˜ŠğŸš€

If you have any more questions or need further details, feel free to ask!"
  
  
### Pre-train dataset
Model Ä‘Ã£ Ä‘Æ°á»£c pre-train trÃªn táº­p dataset *Colossal Clean Crawled Corpus (C4)*.
"""To address these issues, we used the following
heuristics for cleaning up Common Crawlâ€™s web extracted text:
â€¢ We only retained lines that ended in a terminal punctuation mark (i.e. a period,
exclamation mark, question mark, or end quotation mark).
â€¢ We discarded any page with fewer than 3 sentences and only retained lines that
contained at least 5 words.
â€¢ We removed any page that contained any word on the â€œList of Dirty, Naughty, Obscene
or Otherwise Bad Wordsâ€.6
â€¢ Many of the scraped pages contained warnings stating that Javascript should be
enabled so we removed any line with the word Javascript.
â€¢ Some pages had placeholder â€œlorem ipsumâ€ text; we removed any page where the
phrase â€œlorem ipsumâ€ appeared.
â€¢ Some pages inadvertently contained code. Since the curly bracket â€œ{â€ appears in
many programming languages (such as Javascript, widely used on the web) but not in
natural text, we removed any pages that contained a curly bracket.
â€¢ Since some of the scraped pages were sourced from Wikipedia and had citation markers
(e.g. [1], [citation needed], etc.), we removed any such markers.
â€¢ Many pages had boilerplate policy notices, so we removed any lines containing the
strings â€œterms of useâ€, â€œprivacy policyâ€, â€œcookie policyâ€, â€œuses cookiesâ€, â€œuse of
cookiesâ€, or â€œuse cookiesâ€.
â€¢ To deduplicate the data set, we discarded all but one of any three-sentence span
occurring more than once in the data set."""
## Metric  
Metric Ä‘Æ°á»£c sá»­ dá»¥ng lÃ  Rogue Metric.


## Káº¿t quáº£ huáº¥n luyá»‡n  
KhÃ¡ tá»‘t, so sÃ¡nh trá»±c tiáº¿p vá»›i káº¿t quáº£ cá»§a model cÃ³ sáºµn thÃ¬ cho tháº¥y nÃ³ output giá»‘ng nhau.  
Cho tháº¥y quÃ¡ trÃ¬nh huáº¥n luyá»‡n Ä‘Ã£ thÃ nh cÃ´ng. 
![](./report_data/OutputDemo.png)  
*áº¢nh chá»¥p so sÃ¡nh giá»¯a output cá»§a mÃ´ hÃ¬nh huáº¥n luyá»‡n Ä‘Æ°á»£c (3) vÃ  mÃ´ hÃ¬nh cÃ³ sáºµn (2)*  